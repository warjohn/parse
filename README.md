# parse

Этот проект предназначен для парсинга данных с различных веб-страниц с использованием фреймворка Scrapy. Скрипты содержат несколько пауков, каждый из которых отвечает за выполнение определенной задачи все тесты проходились на сайте университета.

## Установка и настройка окружения

Для начала работы с проектом необходимо настроить окружение. Рекомендуется использовать `venv` для создания виртуального окружения:

1. Клонируйте репозиторий на ваш локальный компьютер:

    ```bash
    git clone <URL вашего репозитория>
    cd <название репозитория>
    ```

2. Создайте и активируйте виртуальное окружение:

    ```bash
    python3 -m venv venv
    source venv/bin/activate  # Для Linux/MacOS
    venv\Scripts\activate  # Для Windows
    ```

3. Установите все необходимые зависимости:

    ```bash
    pip install -r requirements.txt
    ```

    Убедитесь, что в `requirements.txt` включены следующие пакеты:

    ```plaintext
    Scrapy
    beautifulsoup4
    psycopg2
    networkx
    ```

## Логика работы пауков

### 1. `AllLinksSpider`

Этот паук ищет все уникальные ссылки на веб-сайте, начиная с заданного URL. Он обходит все страницы в пределах домена и сохраняет найденные ссылки в CSV-файл. Основные моменты:

- Использует `LinkExtractor` для извлечения всех ссылок на странице.
- Проверяет, чтобы каждая ссылка находилась в пределах базового домена и ранее не была сохранена.
- Все найденные ссылки сохраняются в `urls.csv`.

### 2. `EmployeesSpider`

Этот паук извлекает информацию о сотрудниках из баз данных факультетов. Он ищет и парсит страницы сотрудников и сохраняет информацию в базе данных PostgreSQL. Основные моменты:

- Загружает ссылки на страницы сотрудников из базы данных.
- Извлекает информацию, такую как публикации, образование, контактные данные, и сохраняет в базу данных.
- Использует BeautifulSoup для анализа и извлечения данных.

### 3. `DataSpider`

Этот паук извлекает заголовки страниц и контент, расположенный внутри `<main>` на странице. Он также строит график ссылок между страницами с использованием библиотеки NetworkX. Основные моменты:

- Загружает начальные URL из CSV-файла.
- Извлекает текст и ссылки внутри `<main>` на каждой странице.
- Строит и сохраняет график ссылок между страницами.

### 4. `FacultiesSpider`

Этот паук парсит информацию о факультетах с заданного сайта и сохраняет данные в базе данных. Основные моменты:

- Извлекает ссылки, которые соответствуют определенному шаблону.
- Парсит страницу факультета для извлечения названия и основного контента.
- Сохраняет данные в базе данных PostgreSQL.

## Настройки проекта (`settings.py`)

Файл `settings.py` содержит основные настройки проекта Scrapy. Вот ключевые параметры:

- **BOT_NAME**: Имя вашего бота Scrapy.
- **SPIDER_MODULES**: Модули, в которых находятся ваши пауки.
- **NEWSPIDER_MODULE**: Модуль, используемый для создания новых пауков.
- **ROBOTSTXT_OBEY**: Если этот параметр установлен в `True`, Scrapy будет соблюдать инструкции в файле `robots.txt`.
- **DOWNLOAD_DELAY**: Задержка между запросами для предотвращения блокировки со стороны сервера.
- **CONCURRENT_REQUESTS**: Количество одновременных запросов, которые Scrapy может делать.
- **DB_SETTINGS**: Настройки подключения к базе данных PostgreSQL, включая имя базы данных, пользователя, пароль, хост и порт.

Пример:

```python
DB_NAME = 'your_database_name'
DB_USER = 'your_database_user'
DB_PASSWORD = 'your_password'
DB_HOST = 'localhost'
DB_PORT = '5432'
```

## Scrapy Project c нуля

## Установка и настройка окружения

### 1. Установка Python

Убедитесь, что у вас установлен Python версии 3.6 или выше. Вы можете загрузить его с [официального сайта Python](https://www.python.org/downloads/).

### 2. Установка Scrapy

Scrapy можно установить с помощью pip. Выполните следующую команду в терминале:

```bash
pip install scrapy
python -m venv venv
```

Для создания нового проекта Scrapy выполните следующую команду в терминале:
```
scrapy startproject <project_name>
```
Эта команда создает 
```
my_scrapy_project/
    scrapy.cfg            # Основной конфигурационный файл Scrapy
    my_scrapy_project/    # Основная директория вашего проекта
        __init__.py
        items.py          # Определение структур данных (Items) для хранения извлекаемой информации
        middlewares.py    # Определение промежуточного ПО для обработки запросов и ответов
        pipelines.py      # Определение конвейеров (pipelines) для обработки данных перед сохранением
        settings.py       # Основные настройки проекта
        spiders/          # Директория для хранения всех ваших пауков
            __init__.py
```
Создания паука 
```
scrapy genspider <spider_name> <domain>
```
Привет кода 
```Python
import scrapy

class ExampleSpider(scrapy.Spider):
    name = "example_spider"
    allowed_domains = ["example.com"]
    start_urls = ['http://example.com']

    def parse(self, response):
        title = response.xpath('//title/text()').get()
        yield {'title': title}
```
Сохранение результатов 
```
scrapy crawl example_spider
```


Этот `README.md` файл должен помочь пользователям понять, как установить окружение, как работают пауки, и какие настройки необходимо изменить, чтобы адаптировать проект под свои нужды.
